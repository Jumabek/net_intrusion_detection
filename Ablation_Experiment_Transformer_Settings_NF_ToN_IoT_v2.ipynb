{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jumabek/net_intrusion_detection/blob/transformer/Ablation_Experiment_Transformer_Settings_NF_ToN_IoT_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaKCR9NKQNXZ"
      },
      "source": [
        "#Overview\n",
        "##Ablation Study of Transformer Architecture Settings\n",
        "\n",
        "An Ablation Study of Transformer Architecture Settings is a systematic approach to evaluating the impact of various components and configurations within a transformer model. By incrementally adding or removing specific settings or layers, and measuring the effect on performance, this study aims to identify the most influential factors in the model's accuracy and efficiency. The end goal is to fine-tune the transformer architecture to achieve the optimal balance of speed and predictive power, culminating in a well-tuned model with validated settings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyKkMGz5SEML"
      },
      "source": [
        "# Data Preparation for Model Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KC3BF3ZZ9IB0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import io\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset,Subset\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, brier_score_loss, roc_auc_score, average_precision_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s63QwPAqlvY9"
      },
      "source": [
        "## Set a fixed random seed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ybwTHlxWQ4C"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    # Ensuring that PyTorch's convolution operations are deterministic\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWgBlY3HlrcU"
      },
      "source": [
        "## Loading and Preprocessing of Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIK45cNT9IEq",
        "outputId": "ff4aae6c-4ab8-4ee9-99b5-f1cf56574341"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:138: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1JNsyqlCwT8IVudqj3yZ1y6wgbG7me5Pq\n",
            "From (redirected): https://drive.google.com/uc?id=1JNsyqlCwT8IVudqj3yZ1y6wgbG7me5Pq&confirm=t&uuid=03a07d63-25bf-4432-82d1-11c7b8631e2b\n",
            "To: /content/NF-ToN-IoT-v2.zip\n",
            "100% 185M/185M [00:01<00:00, 99.4MB/s]\n",
            "Archive:  /content/NF-ToN-IoT-v2.zip\n",
            "  inflating: NF-ToN-IoT-v2/NetFlow_v2_Features.csv  \n",
            "  inflating: NF-ToN-IoT-v2/NF-ToN-IoT-v2.csv  \n"
          ]
        }
      ],
      "source": [
        "# Downloading the Dataset\n",
        "!gdown --id 1JNsyqlCwT8IVudqj3yZ1y6wgbG7me5Pq\n",
        "!unzip /content/NF-ToN-IoT-v2.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7BW08ZvWhVT"
      },
      "outputs": [],
      "source": [
        "# Loading the NF-ToN-IoT-v2 dataset from a CSV file into a DataFrame. This process takes approximately 46 seconds.\n",
        "df_whole = pd.read_csv(\"/content/NF-ToN-IoT-v2/NF-ToN-IoT-v2.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6m5YT3MPgy8p"
      },
      "outputs": [],
      "source": [
        "# Dropping specific columns from the DataFrame that are not required for the analysis.\n",
        "# These include 'Attack', 'IPV4_SRC_ADDR', 'IPV4_DST_ADDR', and certain byte-related columns.\n",
        "df = df_whole.drop(columns=[\"Attack\",\"IPV4_SRC_ADDR\", \"IPV4_DST_ADDR\",\"SRC_TO_DST_SECOND_BYTES\",\"DST_TO_SRC_SECOND_BYTES\"])\n",
        "\n",
        "# Converting all columns in the DataFrame to float type for consistency and to facilitate numerical operations.\n",
        "df = df.astype(float)\n",
        "\n",
        "# Reducing the dataset size by randomly sampling 0.1% of the data to make the dataset more manageable and speed up computations.\n",
        "# The random state is set to 42 for reproducibility.\n",
        "df = df.sample(frac=0.01, random_state=42)\n",
        "\n",
        "# Extract feature matrix (X) and target vector (y) from the dataframe\n",
        "X = df.iloc[:, :-1].values\n",
        "y = df.iloc[:, -1].values\n",
        "\n",
        "#Splitting the Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "def normalize(data,  mean_i, min_i , max_i ):\n",
        "    \"\"\"\n",
        "    Normalizes the data\n",
        "    Parameters:\n",
        "    - data: The data to be normalized\n",
        "    - mean_i: The mean of the data\n",
        "    - min_i: The minimum value of the data\n",
        "    - max_i: The maximum value of the data\n",
        "    \"\"\"\n",
        "    eps = 1e-15\n",
        "    r = max_i - min_i + eps\n",
        "    data = (data - mean_i) / r\n",
        "\n",
        "    return data\n",
        "\n",
        "mean_i = np.mean(X_train, axis=0)\n",
        "min_i = np.min(X_train, axis=0)\n",
        "max_i = np.max(X_train, axis=0)\n",
        "\n",
        "X_train = normalize(X_train,  mean_i, min_i , max_i )\n",
        "X_test = normalize(X_test,  mean_i, min_i , max_i )\n",
        "\n",
        "# Data Conversion to PyTorch Tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
        "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BG20bvCEl5tN"
      },
      "source": [
        "##Defining necessary functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LflErn8qRyfw"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(test_loader, model, device, name):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "    predicted_probs = []\n",
        "\n",
        "    with torch.no_grad():  # No need to track gradients\n",
        "        for data, labels in test_loader:\n",
        "            data, labels = data.to(device), labels.to(device)  # Move the data to the devic\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "            predicted_labels.extend(predicted.cpu().numpy())\n",
        "            predicted_probs.extend(torch.softmax(outputs, dim=1).cpu().numpy())\n",
        "\n",
        "    predicted_probs = np.array(predicted_probs)\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "\n",
        "    brier_score = brier_score_loss(true_labels, predicted_probs[:, 1])\n",
        "    pr_auc = average_precision_score(true_labels, predicted_probs[:, 1])\n",
        "    roc_auc = roc_auc_score(true_labels, predicted_probs[:, 1])\n",
        "\n",
        "    # Calculate model size\n",
        "    buffer = io.BytesIO()\n",
        "    torch.save(model.state_dict(), buffer)\n",
        "    model_size_kb = buffer.tell() / 1024  # Convert size to kilobytes\n",
        "\n",
        "    metrics = {\n",
        "        name: {\n",
        "            'accuracy': accuracy,\n",
        "            'brier_score': brier_score,\n",
        "            'roc_auc': roc_auc,\n",
        "            'pr_auc': pr_auc,\n",
        "            'model_size_kb': model_size_kb\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rG0T0XRPZW5G"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, criterion, optimizer, num_epochs, device):\n",
        "    model = model.to(device)\n",
        "    for epoch in range(num_epochs):\n",
        "      model.train()\n",
        "      running_loss = 0.00\n",
        "\n",
        "      for i, (data, labels) in enumerate(train_loader):\n",
        "          data, labels = data.cuda(), labels.cuda()\n",
        "          outputs = model(data)\n",
        "          loss = criterion(outputs, labels)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          running_loss += loss.item()\n",
        "\n",
        "      # Calculate and log average metrics for the epoch\n",
        "      average_loss = running_loss / len(train_loader)\n",
        "\n",
        "      print(f\"Epoch [{epoch + 1}/{num_epochs}], Average Loss: {average_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KflcPpFga_Ji"
      },
      "outputs": [],
      "source": [
        "all_metrics = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRXGA7okWzac"
      },
      "source": [
        "#Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeVUkfWCW4Lu"
      },
      "source": [
        "##Experiment 1 on default transformer (pytorch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5QN61poeD5a"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes, d_model, nhead, num_layers, dim_feedforward, dropout):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Linear(input_dim, d_model)\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.unsqueeze(1)  # Add a fake batch dimension for the transformer\n",
        "        x = self.transformer(x, x)  # Encoder-Decoder Self-Attention\n",
        "        x = x.squeeze(1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvDqjWFNRyc1",
        "outputId": "c4ace813-de0e-4bc1-8452-57895d2f7c99"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Average Loss: 1.0334\n",
            "Epoch [2/5], Average Loss: 0.6610\n",
            "Epoch [3/5], Average Loss: 0.6540\n",
            "Epoch [4/5], Average Loss: 0.6534\n",
            "Epoch [5/5], Average Loss: 0.6544\n",
            "[{'Default': {'accuracy': 0.6440377804014168, 'brier_score': 0.23105694884769903, 'roc_auc': 0.5057505399978416, 'pr_auc': 0.6467899725862387, 'model_size_kb': 172578.056640625}}]\n"
          ]
        }
      ],
      "source": [
        "# Set hyperparameters\n",
        "hyperparams = {\n",
        "    'batch_size': 512,\n",
        "    'num_epochs': 5,\n",
        "    'learning_rate': 0.001,\n",
        "    'd_model': 512,\n",
        "    'nhead': 8,\n",
        "    'num_layers': 6,\n",
        "    'dim_feedforward': 2048,\n",
        "    'dropout': 0.1,\n",
        "    'device': 'cuda'\n",
        "}\n",
        "\n",
        "train_loader = DataLoader(train_dataset, hyperparams['batch_size'])\n",
        "test_loader = DataLoader(test_dataset, hyperparams['batch_size'] )\n",
        "\n",
        "# Initialize model\n",
        "model = TransformerModel(\n",
        "    input_dim = X_train.shape[1],\n",
        "    num_classes = len(np.unique([y_train])),\n",
        "    d_model=hyperparams['d_model'],\n",
        "    nhead=hyperparams['nhead'],\n",
        "    num_layers=hyperparams['num_layers'],\n",
        "    dim_feedforward=hyperparams['dim_feedforward'],\n",
        "    dropout=hyperparams['dropout']\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=hyperparams['learning_rate'])\n",
        "\n",
        "# Train and evaluate\n",
        "train_model(model, train_loader, criterion, optimizer, hyperparams['num_epochs'], hyperparams[\"device\"] )\n",
        "metrics = evaluate_model(test_loader, model, hyperparams[\"device\"], name=\"Default\")\n",
        "all_metrics.append(metrics)\n",
        "print(all_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H0uTr_zYwGI"
      },
      "source": [
        "##After changing num_layers = 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSF3RU2aLrGU",
        "outputId": "755a19a6-4498-4a2a-f76a-6346941518e0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Average Loss: 1.0501\n",
            "Epoch [2/5], Average Loss: 0.6573\n",
            "Epoch [3/5], Average Loss: 0.6546\n",
            "Epoch [4/5], Average Loss: 0.6547\n",
            "Epoch [5/5], Average Loss: 0.6546\n"
          ]
        }
      ],
      "source": [
        "# Set hyperparameters\n",
        "hyperparams = {\n",
        "    'batch_size': 512,\n",
        "    'num_epochs': 5,\n",
        "    'learning_rate': 0.001,\n",
        "    'd_model': 512,\n",
        "    'nhead': 8,\n",
        "    'num_layers': 2,\n",
        "    'dim_feedforward': 2048,\n",
        "    'dropout': 0.1,\n",
        "    'device': 'cuda'\n",
        "}\n",
        "\n",
        "train_loader = DataLoader(train_dataset, hyperparams['batch_size'])\n",
        "test_loader = DataLoader(test_dataset, hyperparams['batch_size'] )\n",
        "\n",
        "# Initialize model\n",
        "model1 = TransformerModel(\n",
        "    input_dim = X_train.shape[1],\n",
        "    num_classes = len(np.unique([y_train])),\n",
        "    d_model=hyperparams['d_model'],\n",
        "    nhead=hyperparams['nhead'],\n",
        "    num_layers=hyperparams['num_layers'],\n",
        "    dim_feedforward=hyperparams['dim_feedforward'],\n",
        "    dropout=hyperparams['dropout']\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model1.parameters(), lr=hyperparams['learning_rate'])\n",
        "\n",
        "# Train and evaluate\n",
        "train_model(model1, train_loader, criterion, optimizer, hyperparams['num_epochs'], hyperparams[\"device\"] )\n",
        "metrics = evaluate_model(test_loader, model1, hyperparams[\"device\"], name=\"num_layers = 2\")\n",
        "all_metrics.append(metrics)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCiTbhEnY8Mm"
      },
      "source": [
        "##After Changing d_model = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Mz2rYHWZXUc",
        "outputId": "5ff35f10-b20a-4aa2-b6e9-214a1787ef89"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Average Loss: 0.5771\n",
            "Epoch [2/5], Average Loss: 0.4561\n",
            "Epoch [3/5], Average Loss: 0.3237\n",
            "Epoch [4/5], Average Loss: 0.2718\n",
            "Epoch [5/5], Average Loss: 0.2146\n",
            "{'d_model=16': {'accuracy': 0.9217827626918536, 'brier_score': 0.060095591776708895, 'roc_auc': 0.9681971292331498, 'pr_auc': 0.9762898567029166, 'model_size_kb': 1110.697265625}}\n",
            "{'num_layers = 2': {'accuracy': 0.6440377804014168, 'brier_score': 0.22950178484192701, 'roc_auc': 0.7099174156714138, 'pr_auc': 0.8262721489127085, 'model_size_kb': 57589.009765625}}\n"
          ]
        }
      ],
      "source": [
        "# Set hyperparameters\n",
        "hyperparams = {\n",
        "    'batch_size': 512,\n",
        "    'num_epochs': 5,\n",
        "    'learning_rate': 0.001,\n",
        "    'd_model': 16,\n",
        "    'nhead': 8,\n",
        "    'num_layers': 2,\n",
        "    'dim_feedforward': 2048,\n",
        "    'dropout': 0.1,\n",
        "    'device': 'cuda'\n",
        "}\n",
        "\n",
        "train_loader = DataLoader(train_dataset, hyperparams['batch_size'])\n",
        "test_loader = DataLoader(test_dataset, hyperparams['batch_size'] )\n",
        "\n",
        "# Initialize model\n",
        "model2 = TransformerModel(\n",
        "    input_dim = X_train.shape[1],\n",
        "    num_classes = len(np.unique([y_train])),\n",
        "    d_model=hyperparams['d_model'],\n",
        "    nhead=hyperparams['nhead'],\n",
        "    num_layers=hyperparams['num_layers'],\n",
        "    dim_feedforward=hyperparams['dim_feedforward'],\n",
        "    dropout=hyperparams['dropout']\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model2.parameters(), lr=hyperparams['learning_rate'])\n",
        "\n",
        "# Train and evaluate\n",
        "train_model(model2, train_loader, criterion, optimizer, hyperparams['num_epochs'], hyperparams[\"device\"] )\n",
        "metrics = evaluate_model(test_loader, model2, hyperparams[\"device\"], name=\"d_model=16\")\n",
        "all_metrics.append(metrics)\n",
        "print(all_metrics[-1])\n",
        "print(all_metrics[-2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1t5tMwsbE7ns"
      },
      "source": [
        "##After dim_feedforward = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqePoBCKErzc",
        "outputId": "3578e107-2bed-422d-fa7e-57990f89a9a8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Average Loss: 0.5533\n",
            "Epoch [2/5], Average Loss: 0.4256\n",
            "Epoch [3/5], Average Loss: 0.3669\n",
            "Epoch [4/5], Average Loss: 0.3099\n",
            "Epoch [5/5], Average Loss: 0.2505\n",
            "{'dim_feedforward = 256': {'accuracy': 0.9297520661157025, 'brier_score': 0.05585872371649252, 'roc_auc': 0.9663859893930896, 'pr_auc': 0.9734770802281659, 'model_size_kb': 186.697265625}}\n",
            "{'d_model=16': {'accuracy': 0.9217827626918536, 'brier_score': 0.060095591776708895, 'roc_auc': 0.9681971292331498, 'pr_auc': 0.9762898567029166, 'model_size_kb': 1110.697265625}}\n"
          ]
        }
      ],
      "source": [
        "# Set hyperparameters\n",
        "hyperparams = {\n",
        "    'batch_size': 512,\n",
        "    'num_epochs': 5,\n",
        "    'learning_rate': 0.001,\n",
        "    'd_model': 16,\n",
        "    'nhead': 8,\n",
        "    'num_layers': 2,\n",
        "    'dim_feedforward': 256,\n",
        "    'dropout': 0.1,\n",
        "    'device': 'cuda'\n",
        "}\n",
        "\n",
        "train_loader = DataLoader(train_dataset, hyperparams['batch_size'])\n",
        "test_loader = DataLoader(test_dataset, hyperparams['batch_size'] )\n",
        "\n",
        "# Initialize model\n",
        "model3 = TransformerModel(\n",
        "    input_dim = X_train.shape[1],\n",
        "    num_classes = len(np.unique([y_train])),\n",
        "    d_model=hyperparams['d_model'],\n",
        "    nhead=hyperparams['nhead'],\n",
        "    num_layers=hyperparams['num_layers'],\n",
        "    dim_feedforward=hyperparams['dim_feedforward'],\n",
        "    dropout=hyperparams['dropout']\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model3.parameters(), lr=hyperparams['learning_rate'])\n",
        "\n",
        "# Train and evaluate\n",
        "train_model(model3, train_loader, criterion, optimizer, hyperparams['num_epochs'], hyperparams[\"device\"] )\n",
        "metrics = evaluate_model(test_loader, model3, hyperparams[\"device\"], name=\"dim_feedforward = 256\")\n",
        "all_metrics.append(metrics)\n",
        "print(all_metrics[-1])\n",
        "print(all_metrics[-2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezEZ6CwlZ4dK"
      },
      "source": [
        "##After changing batch_size  = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riUs1uRHaBPh",
        "outputId": "26f90253-5c88-4bce-c424-906686ac6200"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Average Loss: 0.4512\n",
            "Epoch [2/5], Average Loss: 0.3012\n",
            "Epoch [3/5], Average Loss: 0.2319\n",
            "Epoch [4/5], Average Loss: 0.1706\n",
            "Epoch [5/5], Average Loss: 0.1256\n",
            "{'batch_size=128': {'accuracy': 0.9527744982290437, 'brier_score': 0.03950341036614989, 'roc_auc': 0.9837860042895819, 'pr_auc': 0.987048561740365, 'model_size_kb': 186.697265625}}\n",
            "{'dim_feedforward = 256': {'accuracy': 0.9297520661157025, 'brier_score': 0.05585872371649252, 'roc_auc': 0.9663859893930896, 'pr_auc': 0.9734770802281659, 'model_size_kb': 186.697265625}}\n"
          ]
        }
      ],
      "source": [
        "# Set hyperparameters\n",
        "hyperparams = {\n",
        "    'batch_size': 128,\n",
        "    'num_epochs': 5,\n",
        "    'learning_rate': 0.001,\n",
        "    'd_model': 16,\n",
        "    'nhead': 8,\n",
        "    'num_layers': 2,\n",
        "    'dim_feedforward': 256,\n",
        "    'dropout': 0.1,\n",
        "    'device': 'cuda'\n",
        "}\n",
        "\n",
        "train_loader = DataLoader(train_dataset, hyperparams['batch_size'])\n",
        "test_loader = DataLoader(test_dataset, hyperparams['batch_size'] )\n",
        "\n",
        "# Initialize model\n",
        "model4 = TransformerModel(\n",
        "    input_dim = X_train.shape[1],\n",
        "    num_classes = len(np.unique([y_train])),\n",
        "    d_model=hyperparams['d_model'],\n",
        "    nhead=hyperparams['nhead'],\n",
        "    num_layers=hyperparams['num_layers'],\n",
        "    dim_feedforward=hyperparams['dim_feedforward'],\n",
        "    dropout=hyperparams['dropout']\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model4.parameters(), lr=hyperparams['learning_rate'])\n",
        "\n",
        "# Train and evaluate\n",
        "train_model(model4, train_loader, criterion, optimizer, hyperparams['num_epochs'], hyperparams[\"device\"] )\n",
        "metrics = evaluate_model(test_loader, model4, hyperparams[\"device\"], name=\"batch_size=128\")\n",
        "all_metrics.append(metrics)\n",
        "print(all_metrics[-1])\n",
        "print(all_metrics[-2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WLbu-0peqp8"
      },
      "source": [
        "##After nhead = 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEEft2DyetIA",
        "outputId": "e1053bbb-253b-441f-b7c6-04cb5f7ccde2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Average Loss: 0.4577\n",
            "Epoch [2/5], Average Loss: 0.2941\n",
            "Epoch [3/5], Average Loss: 0.2014\n",
            "Epoch [4/5], Average Loss: 0.1518\n",
            "Epoch [5/5], Average Loss: 0.1243\n",
            "{'nhead=2': {'accuracy': 0.9504132231404959, 'brier_score': 0.04024276975882614, 'roc_auc': 0.9826326661832907, 'pr_auc': 0.9880936945251475, 'model_size_kb': 186.697265625}}\n",
            "{'batch_size=128': {'accuracy': 0.9527744982290437, 'brier_score': 0.03950341036614989, 'roc_auc': 0.9837860042895819, 'pr_auc': 0.987048561740365, 'model_size_kb': 186.697265625}}\n"
          ]
        }
      ],
      "source": [
        "# Set hyperparameters\n",
        "hyperparams = {\n",
        "    'batch_size': 128,\n",
        "    'num_epochs': 5,\n",
        "    'learning_rate': 0.001,\n",
        "    'd_model': 16,\n",
        "    'nhead': 2,\n",
        "    'num_layers': 2,\n",
        "    'dim_feedforward': 256,\n",
        "    'dropout': 0.1,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "}\n",
        "\n",
        "train_loader = DataLoader(train_dataset, hyperparams['batch_size'])\n",
        "test_loader = DataLoader(test_dataset, hyperparams['batch_size'] )\n",
        "\n",
        "# Initialize model\n",
        "model5 = TransformerModel(\n",
        "    input_dim = X_train.shape[1],\n",
        "    num_classes = len(np.unique([y_train])),\n",
        "    d_model=hyperparams['d_model'],\n",
        "    nhead=hyperparams['nhead'],\n",
        "    num_layers=hyperparams['num_layers'],\n",
        "    dim_feedforward=hyperparams['dim_feedforward'],\n",
        "    dropout=hyperparams['dropout']\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model5.parameters(), lr=hyperparams['learning_rate'])\n",
        "\n",
        "# Train and evaluate\n",
        "train_model(model5, train_loader, criterion, optimizer, hyperparams['num_epochs'], hyperparams[\"device\"] )\n",
        "metrics = evaluate_model(test_loader, model5, hyperparams[\"device\"], name=\"nhead=2\")\n",
        "all_metrics.append(metrics)\n",
        "print(all_metrics[-1])\n",
        "print(all_metrics[-2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIuVbg1Ym_R4"
      },
      "source": [
        "##Encoder Only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpVzIx6Nm6qg"
      },
      "outputs": [],
      "source": [
        "class TransformerModelEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes, d_model, nhead, num_layers, dim_feedforward, dropout):\n",
        "        super(TransformerModelEncoder, self).__init__()\n",
        "        self.embedding = nn.Linear(input_dim, d_model)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=d_model,\n",
        "                nhead=nhead,\n",
        "                dim_feedforward=dim_feedforward,\n",
        "                dropout=dropout\n",
        "            ),\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.unsqueeze(1)  # Add a fake batch dimension for the transformer\n",
        "        x = self.transformer_encoder(x)  # Pass through the encoder\n",
        "        x = x.squeeze(1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1-y6LS7nQie",
        "outputId": "962ae5bb-2775-45a0-e923-baa3d2516570"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Average Loss: 0.4706\n",
            "Epoch [2/5], Average Loss: 0.2588\n",
            "Epoch [3/5], Average Loss: 0.1641\n",
            "Epoch [4/5], Average Loss: 0.1374\n",
            "Epoch [5/5], Average Loss: 0.1188\n",
            "{'Encoder_only': {'accuracy': 0.961038961038961, 'brier_score': 0.03294386407522342, 'roc_auc': 0.9834118819285789, 'pr_auc': 0.9861840391596766, 'model_size_kb': 87.751953125}}\n",
            "{'nhead=2': {'accuracy': 0.9504132231404959, 'brier_score': 0.04024276975882614, 'roc_auc': 0.9826326661832907, 'pr_auc': 0.9880936945251475, 'model_size_kb': 186.697265625}}\n"
          ]
        }
      ],
      "source": [
        "# Set hyperparameters\n",
        "hyperparams = {\n",
        "    'batch_size': 128,\n",
        "    'num_epochs': 5,\n",
        "    'learning_rate': 0.001,\n",
        "    'd_model': 16,\n",
        "    'nhead': 2,\n",
        "    'num_layers': 2,\n",
        "    'dim_feedforward': 256,\n",
        "    'dropout': 0.1,\n",
        "    'device': 'cuda'\n",
        "}\n",
        "\n",
        "train_loader = DataLoader(train_dataset, hyperparams['batch_size'])\n",
        "test_loader = DataLoader(test_dataset, hyperparams['batch_size'] )\n",
        "\n",
        "# Initialize model\n",
        "model6 = TransformerModelEncoder(\n",
        "    input_dim = X_train.shape[1],\n",
        "    num_classes = len(np.unique([y_train])),\n",
        "    d_model=hyperparams['d_model'],\n",
        "    nhead=hyperparams['nhead'],\n",
        "    num_layers=hyperparams['num_layers'],\n",
        "    dim_feedforward=hyperparams['dim_feedforward'],\n",
        "    dropout=hyperparams['dropout']\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model6.parameters(), lr=hyperparams['learning_rate'])\n",
        "\n",
        "# Train and evaluate\n",
        "train_model(model6, train_loader, criterion, optimizer, hyperparams['num_epochs'], hyperparams[\"device\"] )\n",
        "metrics = evaluate_model(test_loader, model6, hyperparams[\"device\"], name=\"Encoder_only\")\n",
        "all_metrics.append(metrics)\n",
        "print(all_metrics[-1])\n",
        "print(all_metrics[-2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7P1k4LJmYU0"
      },
      "source": [
        "#Evaluation and plots"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}